#!/usr/bin/env python3  
#coding: utf-8


#基于训练好的词向量模型进行聚类
#聚类采用Kmeans算法,并增加类内相似度的计算

import math
import re
import json
import jieba
import numpy as np
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
from collections import defaultdict



#输入模型文件路径
#加载训练好的模型
def load_word2vec_model(path):
    model = Word2Vec.load(path)
    return model

def load_sentence(path):
    sentences = set()  
    with open(path, encoding="utf8") as f:
        for line in f:
            sentence = line.strip()  
            sentences.add(" ".join(jieba.cut(sentence)))  
    print("获取句子数量：", len(sentences))
    return sentences

#将文本向量化
def sentences_to_vectors(sentences, model):
    vectors = []
    for sentence in sentences:   
        words = sentence.split() 
        vector = np.zeros(model.vector_size) 
        for word in words:  
            try:
                vector += model.wv[word]  
            except KeyError:
                vector += np.zeros(model.vector_size)
        vectors.append(vector / len(words))  
    return np.array(vectors)


def main():
    model = load_word2vec_model("model.w2v") 
    sentences = load_sentence("titles.txt")  
    vectors = sentences_to_vectors(sentences, model)   

    n_clusters = int(math.sqrt(len(sentences))) 
    print("指定聚类数量：", n_clusters)

    kmeans = KMeans(n_clusters)  
    kmeans.fit(vectors)         

    sentence_label_dict = defaultdict(list)
    for sentence, label in zip(sentences, kmeans.labels_):  
        sentence_label_dict[label].append(sentence)        

    dis = defaultdict(float)
    for label in sentence_label_dict.keys():
        n = len(sentence_label_dict[label]) 
        dis0 = 0
        for i in range(n): 
            qq = set()
            qq.add(sentence_label_dict[label][i])
            xx = sentences_to_vectors(qq, model)
            yy = kmeans.cluster_centers_[label]
            dis0 += np.sqrt(np.sum(np.square(xx - yy)))
        dis[label] = dis0 / n

    dis_sorted = sorted( dis.values() )  #由小到大排序
    key_sorted = []
    num = 20      # 寻找类内平均距离较小的前num个类
    for i in range(num):  # 寻找类内平均距离较小的前num个类，将类别标签存入key_sorted
        disvalues_list = list(dis.values()) 
        diskeys_list = list(dis.keys())  
        value_ind = disvalues_list.index(dis_sorted[i])  
        key_ind = diskeys_list[value_ind] 
        key_sorted.append(key_ind) 

    print('-----------------------------')
    for i in range(num):  
        print("cluster: %d, 类内平均距离: %f" % (key_sorted[i], dis_sorted[i]))
        nn = len( sentence_label_dict[ key_sorted[i] ] )
        for j in range(min(5, nn)):  
            print(sentence_label_dict[ key_sorted[i] ][j].replace(" ", ""))  
        print('-----------------------------')


if __name__ == "__main__":

    main()

