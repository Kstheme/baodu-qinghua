深度学习基本组件

    线性代数
        标量
        向量
        矩阵

    numpy

    导数
        加（减）法则：[f(x)+g(x)]'=f(x)'+g(x)'
        乘法法则：[f(x)*g(x)]'=f(x)'*g(x)+g(x)'*f(x)
        除法法则：[f(x)/g(x)]'=[f(x)'*g(x)-g(x)'*f(x)]/g(x)2
        链式法则：若h(x)=f(g(x))，则h'(x)=f'(g(x))g'(x)

    梯度：可以理解为多元函数的导数，意义与导数基本一致
        原函数：y = 3x12 + 4x22 + 5x32       
        导函数：y = {6x1 , 8x2 , 10x3}
        在[1，1，1]处的梯度是[6，8，10]
        梯度是个向量

    梯度下降法：
        SGD（Stochastic gradient descent：随机梯度下降）
        根据梯度，更新权重
        学习率控制权重更新的幅度
        
    权重更新方式
        Gradient descent
        所有样本一起计算梯度（累加）

        Stochastic gradient descent
        每次使用一个样本计算梯度

        Mini-batch gradient descent
        每次使用n个样本计算梯度（累加）

    深度学习
        损失函数越小，模型越好

        学习的目标是损失函数最小化

        模型权重影响损失函数值

        通过梯度下降来找到最优权重

    pytorch

    全连接层（线性层）
        决定了隐含层输出的维度，一般称为隐单元个数（hidden size）

    激活函数
        为模型添加非线性因素，使模型具有拟合非线性函数的能力
        无激活函数时 y = w1(w2(w3 * x + b3) +b2) + b1 仍然是线性函数

        Sigmoid
            一种非线性映射，将任意输入映射到0-1之间

            缺点：
            1.计算耗时，包含指数运算
            2.非0均值，会导致收敛慢
            3.易造成梯度消失

        tanh
            以0为均值，解决了sigmoid的一定缺点
            但是依然存在梯度消失问题
            计算同样非常耗时

        Relu = max(0,x)
            在正区间不易发生梯度消失
            计算速度非常快
            一定程度上降低过拟合的风险

        Softmax

    损失函数
        均方差

        交叉熵

        指数损失

        对数损失

        0/1损失

        Hinge损失（二分类）

    优化器-Adam





